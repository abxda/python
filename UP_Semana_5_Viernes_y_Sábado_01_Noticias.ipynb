{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqBXI2wgqGjrqrYW8howd6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/python/blob/main/UP_Semana_5_Viernes_y_S%C3%A1bado_01_Noticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnaMzyI-OTrm"
      },
      "outputs": [],
      "source": [
        "!pip install -U duckduckgo_search\n",
        "!pip install wordcloud matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Descargar e instalar el modelo en español\n",
        "!python -m spacy download es_core_news_sm\n"
      ],
      "metadata": {
        "id": "34eb2NzLcdi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from duckduckgo_search import AsyncDDGS\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "zb9lFsKwPz4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configuraciones para la búsqueda de noticias en Aguascalientes\n",
        "configuraciones = [\n",
        "    {\"idioma\": \"Español\", \"keywords\": \"Aguascalientes\", \"region\": \"mx-es\"}\n",
        "]\n",
        "\n",
        "# Diccionario para almacenar los resultados de noticias por idioma\n",
        "resultados_noticias = {\"Español\": []}\n",
        "\n",
        "async def buscar_noticias(idioma, keywords, region):\n",
        "    async_ddgs = AsyncDDGS()\n",
        "    noticias = await async_ddgs.news(keywords=keywords, region=region, safesearch='moderate', max_results=100)\n",
        "    return idioma, noticias\n",
        "\n",
        "async def main():\n",
        "    tasks = [buscar_noticias(conf['idioma'], conf['keywords'], conf['region']) for conf in configuraciones]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    for idioma, noticias in results:\n",
        "        resultados_noticias[idioma].extend(noticias)  # Añadir noticias a la lista correspondiente del idioma\n",
        "        print(f\"Resultados para {idioma}: {len(noticias)} noticias encontradas.\")\n",
        "\n",
        "    # Imprimir las listas de noticias por idioma\n",
        "    for idioma, lista_noticias in resultados_noticias.items():\n",
        "        print(f\"\\n{idioma} ({len(lista_noticias)} noticias):\")\n",
        "        print([noticia['title'] for noticia in lista_noticias[:10]])  # Mostrar solo los títulos de las primeras 10 noticias para brevedad\n",
        "\n",
        "    # Opcional: Concatenar títulos de noticias y generar nube de palabras\n",
        "    news_text = ' '.join([news['body'] for news in resultados_noticias['Español']])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(news_text)\n",
        "\n",
        "    # Mostrar la nube de palabras\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Luego en otra celda de Jupyter:\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "zSBH9NZfP1Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(resultados_noticias[\"Español\"])"
      ],
      "metadata": {
        "id": "8pLf-PBNQR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noticias = pd.DataFrame(resultados_noticias[\"Español\"])"
      ],
      "metadata": {
        "id": "zWADNh9WQ3WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noticias"
      ],
      "metadata": {
        "id": "HzNrtotTRluZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noticias.columns"
      ],
      "metadata": {
        "id": "ANtWfeUVSeT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero, asegúrate de descargar las stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "oUR66vhLXB_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YhDAC-eQbad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noticias['clean_body'] = df_noticias['body'].str.lower()\n",
        "df_noticias['clean_body'] = df_noticias['clean_body'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "df_noticias['clean_body'] = df_noticias['clean_body'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
      ],
      "metadata": {
        "id": "2Nca1QkfUeyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_nube_de_palabras(textos_limpios):\n",
        "\n",
        "    # Unir todos los cuerpos de texto en una sola cadena\n",
        "    textos_unidos = ' '.join(textos_limpios)\n",
        "\n",
        "    # Crear la nube de palabras\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(textos_unidos)\n",
        "\n",
        "    # Mostrar la nube de palabras\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WJ9iLqzuWzOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generar_nube_de_palabras(df_noticias['clean_body'])"
      ],
      "metadata": {
        "id": "7WZNPKiBYaeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Contar noticias por fuente\n",
        "conteo_fuentes = df_noticias['source'].value_counts()\n",
        "\n",
        "# Crear gráfica de barras\n",
        "plt.figure(figsize=(12, 8))  # Ajusta el tamaño para mejor visualización\n",
        "conteo_fuentes.plot(kind='bar', color='skyblue')  # Puedes cambiar el color si deseas\n",
        "plt.title('Cantidad de Noticias por Fuente')\n",
        "plt.xlabel('Fuente')\n",
        "plt.ylabel('Cantidad de Noticias')\n",
        "plt.xticks(rotation=90)  # Rota las etiquetas del eje x a 90 grados\n",
        "plt.tight_layout()  # Ajusta automáticamente los parámetros de la subtrama para dar espacio al contenido\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wY2BMFx2Ye6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Tokenización y lematización\n",
        "df_noticias['lemmatized'] = df_noticias['clean_body'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))"
      ],
      "metadata": {
        "id": "DMQtgY8tZlQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generar_nube_de_palabras(df_noticias['lemmatized'])"
      ],
      "metadata": {
        "id": "YACWEaQVbdAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "pmPahqkGcsEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesar el texto\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "df_noticias['processed'] = df_noticias['body'].apply(preprocess)\n"
      ],
      "metadata": {
        "id": "LUTvTXTIdEaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización con TF-IDF usando n-gramas de caracteres de 3 a 5 longitudes\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
        "X = vectorizer.fit_transform(df_noticias['processed'])\n",
        "\n",
        "# Normalización de vectores\n",
        "X_normalized = normalize(X)"
      ],
      "metadata": {
        "id": "naYnZvf4eSeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering\n",
        "db = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
        "clusters = db.fit_predict(X_normalized)"
      ],
      "metadata": {
        "id": "od0RNZ7hgNDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering con K-means\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_normalized)"
      ],
      "metadata": {
        "id": "PbTcp7-ogK3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_noticias['cluster'] = clusters\n",
        "\n",
        "# Análisis de clusters\n",
        "cluster_counts = df_noticias['cluster'].value_counts()\n",
        "\n",
        "# Imprimir los resultados de los clusters más grandes\n",
        "print(cluster_counts)\n"
      ],
      "metadata": {
        "id": "H1nR-mZ_eUaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters"
      ],
      "metadata": {
        "id": "HlQQelHoej73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def generar_nube_de_palabras(textos_limpios, ax):\n",
        "    # Unir todos los cuerpos de texto en una sola cadena\n",
        "    textos_unidos = ' '.join(textos_limpios)\n",
        "\n",
        "    # Crear la nube de palabras\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(textos_unidos)\n",
        "\n",
        "    # Mostrar la nube de palabras en el eje especificado\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Crear una figura con subplots\n",
        "fig, axs = plt.subplots(5, 2, figsize=(20, 50))  # 5 filas, 2 columnas\n",
        "\n",
        "# Generar una nube de palabras y listar títulos para cada cluster\n",
        "for i in range(5):\n",
        "    # Filtrar el DataFrame para obtener textos y títulos del cluster actual\n",
        "    cluster_data = df_noticias[df_noticias['cluster'] == i]\n",
        "    textos_cluster = cluster_data['processed'].tolist()\n",
        "    titulos_cluster = cluster_data[['title', 'source']].apply(lambda x: f\"{x['title']} ({x['source']})\", axis=1).tolist()\n",
        "\n",
        "    if textos_cluster:  # Verificar si la lista no está vacía\n",
        "        generar_nube_de_palabras(textos_cluster, axs[i, 0])\n",
        "        # Poner títulos en la segunda columna\n",
        "        axs[i, 1].text(0.5, 0.5, \"\\n\".join(titulos_cluster), verticalalignment='center', horizontalalignment='left', fontsize=12, family='monospace')\n",
        "        axs[i, 1].axis('off')\n",
        "\n",
        "    axs[i, 0].set_title(f'Cluster {i+1} - Nube de Palabras')\n",
        "    axs[i, 1].set_title(f'Cluster {i+1} - Títulos y Fuentes')\n",
        "\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kDBvIjcAenPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6FtpW7WgqTP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}